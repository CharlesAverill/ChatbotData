{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shoshannah.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl-mJL1zGtro",
        "colab_type": "text"
      },
      "source": [
        "# Clone the git repositories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKXJ8m5iGoUK",
        "colab_type": "code",
        "outputId": "5e3f3647-889b-4952-f86b-43ada98f7126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/JRC1995/Chatbot.git\n",
        "!git clone https://github.com/CharlesAverill/ChatbotData.git"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'Chatbot' already exists and is not an empty directory.\n",
            "fatal: destination path 'ChatbotData' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzeBZSyiG6F5",
        "colab_type": "text"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG6YkBtjG5SQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d64da8ef-414e-4514-f06e-b47d7cb78740"
      },
      "source": [
        "!apt-get install ffmpeg libavcodec-extra libomp-dev"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (5.0.1-1).\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "libavcodec-extra is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05nKd88uwhLA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "2eecbc79-95d6-4224-ed44-9041a13e5d79"
      },
      "source": [
        "!python -m pip install torch unidecode pydub faiss tensorflow-text transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.6/dist-packages (0.24.0)\n",
            "Requirement already satisfied: faiss in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: tensorflow<2.3,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (2.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (1.28.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (0.9.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (2.2.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (3.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (1.6.0.post3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F-Q3wwhYp3a",
        "colab_type": "text"
      },
      "source": [
        "# Download model files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYL6szhzYrW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ConvRT\n",
        "!wget -O /content/ConvRT.tar.gz \"http://models.poly-ai.com/multi_context_convert/v1/model.tar.gz\"\n",
        "!tar -C /content/Chatbot/Sentence_Encoder/Embeddings/ConvRT/ -zxvf /content/ConvRT.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzNZVWJcwkXx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "c9843ff6-028c-4bb4-b720-8affccd774e2"
      },
      "source": [
        "# Universal Sentence Encoder QA\n",
        "!wget -O /content/USE_QA.tar.gz \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3?tf-hub-format=compressed\"\n",
        "!tar -C /content/Chatbot/Sentence_Encoder/Embeddings/USE_QA/ -zxvf /content/USE_QA.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-13 16:33:16--  https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3?tf-hub-format=compressed\n",
            "Resolving tfhub.dev (tfhub.dev)... 74.125.142.102, 74.125.142.101, 74.125.142.139, ...\n",
            "Connecting to tfhub.dev (tfhub.dev)|74.125.142.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://storage.googleapis.com/tfhub-modules/google/universal-sentence-encoder-multilingual-qa/3.tar.gz [following]\n",
            "--2020-05-13 16:33:17--  https://storage.googleapis.com/tfhub-modules/google/universal-sentence-encoder-multilingual-qa/3.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 2607:f8b0:400e:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 325767668 (311M) [application/x-tar]\n",
            "Saving to: ‘/content/USE_QA.tar.gz’\n",
            "\n",
            "/content/USE_QA.tar 100%[===================>] 310.68M  84.2MB/s    in 3.7s    \n",
            "\n",
            "2020-05-13 16:33:21 (84.2 MB/s) - ‘/content/USE_QA.tar.gz’ saved [325767668/325767668]\n",
            "\n",
            "./\n",
            "./assets/\n",
            "./variables/\n",
            "./variables/variables.index\n",
            "./variables/variables.data-00000-of-00001\n",
            "./saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOgmRV6EwmfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b56a9a87-d633-4148-9de2-71cba7a63c0b"
      },
      "source": [
        "!rm /content/ConvRT.tar.gz\n",
        "!rm /content/USE_QA.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/ConvRT.tar.gz': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBjPIJ7QbpE",
        "colab_type": "text"
      },
      "source": [
        "# Combine the split data files and run setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juWj73CaQeIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Reddit Data\n",
        "%cd /content/ChatbotData/Random_Reddit_Data\n",
        "!cat * > rrd.zip\n",
        "!unzip rrd.zip\n",
        "!rsync -av Random_Reddit_Data /content/Chatbot/Scripted/\n",
        "\n",
        "%cd /content/Chatbot/Scripted/\n",
        "!python setup.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a79PvvvSwpnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Retriever Data\n",
        "%cd /content/ChatbotData/Data\n",
        "!cat * > retrieverdata.zip\n",
        "!unzip retrieverdata.zip\n",
        "!rsync -av Data /content/Chatbot/Retriever/Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSZrVtkPwrS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reddit Database file\n",
        "%cd /content/ChatbotData/Database\n",
        "!cat * > reddit.db\n",
        "!mkdir /content/Chatbot/Retriever/Database/\n",
        "!mv reddit.db /content/Chatbot/Retriever/Database/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STMM2BOuwsvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Faiss Index\n",
        "%cd /content/ChatbotData/Faiss_index\n",
        "!cat * > large.index\n",
        "!mv large.index /content/Chatbot/Retriever/Faiss_index/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waJHBAfEw2Na",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6e9fa75e-1277-4d25-c416-13761d34c2e2"
      },
      "source": [
        "# Classifier data\n",
        "%cd /content/ChatbotData/\n",
        "!mv processed_data.pkl /content/Chatbot/Classifier/data/"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ChatbotData\n",
            "mv: cannot stat 'processed_data.pkl': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdplTsGFwuCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42017f1f-c524-4a8e-8578-b1a49d1bb243"
      },
      "source": [
        "# Setup scripts\n",
        "%cd /content/Chatbot/Retriever/\n",
        "!python fill_data.py\n",
        "!python faiss_it.py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Chatbot/Retriever\n",
            "2020-05-13 16:31:31.721397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-13 16:31:36.401104: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n",
            "2020-05-13 16:31:36.404262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c5d640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-05-13 16:31:36.404307: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-05-13 16:31:36.424119: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-05-13 16:31:36.554857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:36.555921: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c5dd40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-05-13 16:31:36.555956: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-05-13 16:31:36.556174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:36.556706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-05-13 16:31:36.556753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-13 16:31:36.571491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-05-13 16:31:36.580264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-05-13 16:31:36.588384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-05-13 16:31:36.604289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-05-13 16:31:36.610507: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-05-13 16:31:37.088318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-05-13 16:31:37.088601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:37.089358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:37.089969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-05-13 16:31:37.093659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-13 16:31:37.655317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-05-13 16:31:37.655380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2020-05-13 16:31:37.655395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2020-05-13 16:31:37.662251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:37.663062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:37.663699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Traceback (most recent call last):\n",
            "  File \"fill_data.py\", line 16, in <module>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 102, in load\n",
            "    obj = tf_v1.saved_model.load_v2(module_path, tags=tags)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 578, in load\n",
            "    return load_internal(export_dir, tags)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 604, in load_internal\n",
            "    export_dir)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 123, in __init__\n",
            "    self._load_all()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 141, in _load_all\n",
            "    self._setup_functions_captures()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 190, in _setup_functions_captures\n",
            "    for node_id in proto.bound_inputs]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 190, in <listcomp>\n",
            "    for node_id in proto.bound_inputs]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 228, in _get_tensor_from_node\n",
            "    with ops.init_scope():\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\n",
            "    return next(self.gen)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5520, in init_scope\n",
            "    with outer_context(), name_scope(\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3768, in as_default\n",
            "    return _default_graph_stack.get_controller(self)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 159, in helper\n",
            "    return _GeneratorContextManager(func, args, kwds)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 60, in __init__\n",
            "    self.gen = func(*args, **kwds)\n",
            "KeyboardInterrupt\n",
            "2020-05-13 16:31:48.070740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.071365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-05-13 16:31:48.071455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-13 16:31:48.071538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-05-13 16:31:48.071574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-05-13 16:31:48.071607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-05-13 16:31:48.071645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-05-13 16:31:48.071669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-05-13 16:31:48.071697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-05-13 16:31:48.071806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.072403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.072917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-05-13 16:31:48.073546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.074132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-05-13 16:31:48.074170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-13 16:31:48.074206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-05-13 16:31:48.074236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-05-13 16:31:48.074255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-05-13 16:31:48.074290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-05-13 16:31:48.074312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-05-13 16:31:48.074337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-05-13 16:31:48.074443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.075047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.075649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-05-13 16:31:48.075696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-05-13 16:31:48.075724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2020-05-13 16:31:48.075735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2020-05-13 16:31:48.075837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.076534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-13 16:31:48.077076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "tcmalloc: large alloc 1272578048 bytes == 0x5f3c8000 @  0x7f0bad4821e7 0x7f0baafe85e1 0x7f0bab04cc78 0x7f0bab04fdb8 0x7f0bab050395 0x7f0bab0e765d 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0bad07fb97 0x5b250a\n",
            "Traceback (most recent call last):\n",
            "  File \"faiss_it.py\", line 19, in <module>\n",
            "    queries = np.asarray(queries).reshape((-1, 1024))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
            "    return array(a, dtype, copy=False, order=order)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaxOjLoeNv8",
        "colab_type": "text"
      },
      "source": [
        "# More Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOW2zsOcePWn",
        "colab_type": "code",
        "outputId": "18bb17ba-3358-415c-99f3-601fcbbbb2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# DialoGPT download\n",
        "%cd /content/Chatbot/Generator/DialoGPT/\n",
        "!mkdir Parameters\n",
        "!wget -O ./Parameters/medium_ft.pkl https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
        "!wget -O ./Parameters/small_reverse.pkl https://convaisharables.blob.core.windows.net/lsp/multiref/small_reverse.pkl"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Chatbot/Generator/DialoGPT\n",
            "mkdir: cannot create directory ‘Parameters’: File exists\n",
            "--2020-05-13 16:51:00--  https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862954531 (823M) [application/octet-stream]\n",
            "Saving to: ‘./Parameters/medium_ft.pkl’\n",
            "\n",
            "./Parameters/medium 100%[===================>] 822.98M  69.0MB/s    in 11s     \n",
            "\n",
            "2020-05-13 16:51:11 (77.5 MB/s) - ‘./Parameters/medium_ft.pkl’ saved [862954531/862954531]\n",
            "\n",
            "--2020-05-13 16:51:21--  https://convaisharables.blob.core.windows.net/lsp/multiref/small_reverse.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862954531 (823M) [application/octet-stream]\n",
            "Saving to: ‘./Parameters/small_reverse.pkl’\n",
            "\n",
            "./Parameters/small_ 100%[===================>] 822.98M  49.8MB/s    in 14s     \n",
            "\n",
            "2020-05-13 16:51:35 (59.0 MB/s) - ‘./Parameters/small_reverse.pkl’ saved [862954531/862954531]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYPlmG4qwybU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d4d1c342-498b-44e2-a800-2cce14b00d40"
      },
      "source": [
        "# TTS model\n",
        "%cd /content/ChatbotData/\n",
        "!rsync -av tts_model /content/Chatbot/TTS/"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ChatbotData\n",
            "sending incremental file list\n",
            "\n",
            "sent 119 bytes  received 17 bytes  272.00 bytes/sec\n",
            "total size is 96,688,770  speedup is 710,946.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH8IBtbbO_za",
        "colab_type": "text"
      },
      "source": [
        "# Interact"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFciYY5vPFt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f1c9b339-8624-48f9-97f4-8606c06960d5"
      },
      "source": [
        "%cd /content/Chatbot/\n",
        "from TTS.text2speech import tts_class\n",
        "from multiprocessing import Process\n",
        "import faiss\n",
        "import time\n",
        "import sqlite3\n",
        "import csv\n",
        "import random\n",
        "import copy\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import math\n",
        "import numpy as np\n",
        "import pickle\n",
        "from Retriever.Retrieve import retrieve\n",
        "import Utils.functions as utils\n",
        "from ReRanker.rerank import rank_and_choose\n",
        "from Generator.generator import generate as DialoGPT_Generate\n",
        "from Classifier.model.dialog_acts import Encoder as Classifier\n",
        "from Sentence_Encoder.meta_response_encoder_fast import encode as response_encode\n",
        "from Sentence_Encoder.meta_query_encoder_fast import encode as query_encode\n",
        "import Sentence_Encoder.encoder_client as encoder_client\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch as T\n",
        "import os\n",
        "import sys\n",
        "\"\"\"\n",
        "import argparse\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "logging.basicConfig(level=logging.CRITICAL)\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Chatbot\")\n",
        "parser.add_argument('--voice', dest='voice', action='store_true')\n",
        "parser.add_argument('--no-voice', dest='voice', action='store_false')\n",
        "parser.set_defaults(voice=True)\n",
        "flags = parser.parse_args()\n",
        "\"\"\"\n",
        "device = \"cuda\"\n",
        "\n",
        "with open(\"Retriever/Faiss_index/thread_idx.pkl\", 'rb') as fp:\n",
        "    idx = pickle.load(fp)\n",
        "index = faiss.read_index('Retriever/Faiss_index/large.index')\n",
        "# LOAD DATABASE\n",
        "print(\"Loading database...\")\n",
        "conn = sqlite3.connect('Retriever/Database/reddit.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "\n",
        "# LOAD SCRIPTS\n",
        "print(\"Loading scripts...\")\n",
        "with open('Scripted/Processed_Scripts/Bot_Profile.pkl', 'rb') as fp:\n",
        "    bot_profile = pickle.load(fp)\n",
        "\n",
        "bot_queries = [k for k, v in bot_profile.items()]\n",
        "\n",
        "with open('Scripted/Processed_Scripts/Chatterbot.pkl', 'rb') as fp:\n",
        "    chatterbot = pickle.load(fp)\n",
        "\n",
        "chatterbot_queries = [k for k, v in chatterbot.items()]\n",
        "\n",
        "# LOAD SCRIPT EMBEDDINGS\n",
        "print(\"Loading script embeddings...\")\n",
        "with open('Scripted/Processed_Scripts/embedded_bot_queries.pkl', 'rb') as fp:\n",
        "    bot_queries_embd = pickle.load(fp)\n",
        "\n",
        "with open('Scripted/Processed_Scripts/embedded_chatterbot_queries.pkl', 'rb') as fp:\n",
        "    chatterbot_queries_embd = pickle.load(fp)\n",
        "\n",
        "# Load Dialog Acts Classifer\n",
        "print(\"Loading Dialog Acts Classifier...\")\n",
        "with open(\"Classifier/data/processed_data.pkl\", \"rb\") as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "labels2idx = data[\"labels2idx\"]\n",
        "idx2labels = {v: k for k, v in labels2idx.items()}\n",
        "\n",
        "with T.no_grad():\n",
        "    dialog_act_classifier = Classifier(\n",
        "        D=bot_queries_embd.shape[-1], classes_num=len(labels2idx)).cuda()\n",
        "    checkpoint = T.load(\"Classifier/Model_Backup/model.pt\")\n",
        "    dialog_act_classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "    dialog_act_classifier = dialog_act_classifier.eval()\n",
        "\n",
        "# Load TTS model\n",
        "with T.no_grad():\n",
        "    text2speech = tts_class()\n",
        "\n",
        "# LOAD DialoGPT Generator\n",
        "print(\"Loading DialoGPT Generator...\")\n",
        "with T.no_grad():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('Generator/DialoGPT/Configs/')\n",
        "    weights = T.load('Generator/DialoGPT/Parameters/medium_ft.pkl')\n",
        "    weights_reverse = T.load('Generator/DialoGPT/Parameters/small_reverse.pkl')\n",
        "    cfg = GPT2Config.from_json_file('Generator/DialoGPT/Configs/config.json')\n",
        "    model = GPT2LMHeadModel(cfg)\n",
        "    model_reverse = GPT2LMHeadModel(cfg)\n",
        "\n",
        "    # fix misused key value\n",
        "    weights[\"lm_head.weight\"] = weights[\"lm_head.decoder.weight\"]\n",
        "    weights.pop(\"lm_head.decoder.weight\", None)\n",
        "    weights_reverse[\"lm_head.weight\"] = weights_reverse[\"lm_head.decoder.weight\"]\n",
        "    weights_reverse.pop(\"lm_head.decoder.weight\", None)\n",
        "\n",
        "    model.load_state_dict(weights,strict=False) \n",
        "    model.to('cuda') \n",
        "    model.eval() \n",
        "\n",
        "    model_reverse.load_state_dict(weights_reverse,strict=False) \n",
        "    model_reverse.to('cuda') \n",
        "    model_reverse.eval() \n",
        "print(\"Finished Loading\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Chatbot\n",
            "Loading database...\n",
            "Loading scripts...\n",
            "Loading script embeddings...\n",
            "Loading Dialog Acts Classifier...\n",
            "Loading DialoGPT Generator...\n",
            "Finished Loading\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIqErXou6agl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "747114d6-0c6d-46e6-a224-8aae98db5a79"
      },
      "source": [
        "with tf.device(\"/gpu:0\"):\n",
        "    # Hub Models\n",
        "\n",
        "    ConvRT_model = encoder_client.EncoderClient(\n",
        "        \"Sentence_Encoder/Embeddings/ConvRT\", use_extra_context=True)\n",
        "    USE_QA_model = hub.load(\"Sentence_Encoder/Embeddings/USE_QA/\")\n",
        "\n",
        "    # %%\n",
        "\n",
        "    command_codes = [\"<PASS>\", \"<JOKE>\", \"<GENERATE>\",\n",
        "                      \"<INITIATE>\", \"<TIL>\", \"<STORY>\", \"<SHOWER>\", \"<STOP>\"]\n",
        "\n",
        "    code_map = {\"<INITIATE>\": [\"Scripted/Random_Reddit_Data/nostupidq.csv\",\n",
        "                                \"Scripted/Random_Reddit_Data/jokesq.csv\",\n",
        "                                \"Scripted/Random_Reddit_Data/showerthoughtsq.csv\",\n",
        "                                \"Scripted/Random_Reddit_Data/tilq.csv\"],\n",
        "                \"<TIL>\": [\"Scripted/Random_Reddit_Data/tilq.csv\"],\n",
        "                \"<SHOWER>\": [\"Scripted/Random_Reddit_Data/showerthoughtsq.csv\"],\n",
        "                \"<STORY>\": [\"Scripted/Random_Reddit_Data/writingpromptsa.csv\"],\n",
        "                \"<JOKE>\": [\"Scripted/Random_Reddit_Data/jokesq.csv\"]}\n",
        "\n",
        "    def random_response(candidates, conversation_history, p=None):\n",
        "        loop = 5\n",
        "\n",
        "        if p is None:\n",
        "            response = random.choice(candidates)\n",
        "        else:\n",
        "            response = np.random.choice(candidates, p=p)\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        while response in conversation_history:\n",
        "            if p is None:\n",
        "                response = random.choice(candidates)\n",
        "            else:\n",
        "                response = np.random.choice(candidates, p=p)\n",
        "            i += 1\n",
        "            if i > loop:\n",
        "                break\n",
        "\n",
        "        return response\n",
        "\n",
        "    # %%\n",
        "\n",
        "    def load_random_reddit(directory, conversation_history):\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "        with open(directory, newline='') as csvfile:\n",
        "            csv_reader = csv.DictReader(csvfile)\n",
        "            for i, row in enumerate(csv_reader):\n",
        "                if 'writing' in directory:\n",
        "                    parent_id = str(row['parent_id'])[3:]\n",
        "                    thread_id = str(row['link_id'])[3:]\n",
        "                    if parent_id == thread_id:\n",
        "                        candidate = str(row[\"body\"])\n",
        "                else:\n",
        "                    candidate = str(row[\"title\"])\n",
        "                    if 'joke' in directory:\n",
        "                        candidate += \".... \"+str(row['selftext'])\n",
        "                candidates.append(candidate)\n",
        "\n",
        "        return random_response(candidates, conversation_history)\n",
        "\n",
        "    # extract top candidates (queries or responses)\n",
        "\n",
        "    def top_candidates(candidates, scores, top=1):\n",
        "        sorted_score_idx = np.flip(np.argsort(scores), axis=-1)\n",
        "        candidates = [candidates[i] for i in sorted_score_idx.tolist()]\n",
        "        scores = [scores[i] for i in sorted_score_idx.tolist()]\n",
        "        return candidates[0:top], scores[0:top], sorted_score_idx.tolist()\n",
        "\n",
        "    # %%\n",
        "\n",
        "    def generate(texts, past):\n",
        "        candidates, _ = DialoGPT_Generate(texts, model, tokenizer)\n",
        "        return candidates, past\n",
        "\n",
        "    # START DOING STUFF\n",
        "\n",
        "    conversation_history = []\n",
        "    past = None\n",
        "    stop_flag = 0\n",
        "    print(\"\\n\")\n",
        "\n",
        "    while True:\n",
        "\n",
        "        utterance = input(\"Say Something: \")  # ,hello how are ya today\"\n",
        "\n",
        "        utils.delay_print(\"\\nThinking......\")\n",
        "\n",
        "        candidates = []\n",
        "        temp_candidates = []\n",
        "        temp_scores = []\n",
        "\n",
        "        if not conversation_history:\n",
        "            query_context = []\n",
        "            response_context = [\"\"]\n",
        "        else:\n",
        "            if len(conversation_history) > 5:\n",
        "                truncated_history = copy.deepcopy(conversation_history[-5:])\n",
        "            else:\n",
        "                truncated_history = copy.deepcopy(conversation_history)\n",
        "\n",
        "            response_context = [conversation_history[-1]]\n",
        "\n",
        "            # ConveRT needs reversed Context, not sure about USE QA but assuming it's not reverse\n",
        "\n",
        "            query_context = [stuff for stuff in truncated_history]\n",
        "\n",
        "        query_encoding = query_encode([utterance], USE_QA_model, ConvRT_model, [query_context])\n",
        "\n",
        "        if conversation_history:\n",
        "            if len(conversation_history) > 5:\n",
        "                truncated_history = conversation_history[-5:]\n",
        "            else:\n",
        "                truncated_history = conversation_history\n",
        "            generated_responses, past = generate(truncated_history+[utterance], past)\n",
        "        else:\n",
        "            generated_responses, past = generate([utterance], past)\n",
        "\n",
        "        bot_cosine_scores = utils.cosine_similarity_nd(query_encoding, bot_queries_embd)\n",
        "        bot_queries_, bot_cosine_scores_, _ = top_candidates(bot_queries, bot_cosine_scores, top=1)\n",
        "\n",
        "        active_codes = []\n",
        "\n",
        "        bot_candidates = bot_profile[bot_queries_[0]]\n",
        "\n",
        "        filtered_bot_candidates = []\n",
        "        for candidate in bot_candidates:\n",
        "            flag = 0\n",
        "            for code in command_codes:\n",
        "                if code in candidate:\n",
        "                    active_codes.append(code)\n",
        "                    candidate = candidate.replace(code, \"\")\n",
        "                    filtered_bot_candidates.append(candidate)\n",
        "                    flag = 1\n",
        "                    break\n",
        "\n",
        "            if flag == 0:\n",
        "                candidates.append(candidate)\n",
        "                filtered_bot_candidates.append(candidate)\n",
        "                active_codes.append(\"\")\n",
        "\n",
        "        with T.no_grad():\n",
        "            logits = dialog_act_classifier(T.tensor(query_encoding).to(device))\n",
        "            _, sorted_idx = T.sort(logits, dim=-1, descending=True)\n",
        "            sorted_idx = sorted_idx.squeeze(0)\n",
        "            sorted_idx = sorted_idx[0:2].cpu().tolist()\n",
        "\n",
        "        labels = [idx2labels[i] for i in sorted_idx]\n",
        "\n",
        "        # print(labels)\n",
        "\n",
        "        \"\"\"\n",
        "        Possible Dialog Acts:\n",
        "        ['nonsense', 'dev_command', 'open_question_factual', 'appreciation', 'other_answers', 'statement', \\\n",
        "        'respond_to_apology', 'pos_answer', 'closing', 'comment', 'neg_answer', 'yes_no_question', 'command', \\\n",
        "        'hold', 'NULL', 'back-channeling', 'abandon', 'opening', 'other', 'complaint', 'opinion', 'apology', \\\n",
        "        'thanking', 'open_question_opinion']\n",
        "        \"\"\"\n",
        "\n",
        "        if bot_cosine_scores_[0] >= 0.75:\n",
        "            response, id = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                            tokenizer,\n",
        "                                            model_reverse,\n",
        "                                            utterance,\n",
        "                                            query_encoding,\n",
        "                                            filtered_bot_candidates,\n",
        "                                            response_context,\n",
        "                                            conversation_history)\n",
        "            code = active_codes[id]\n",
        "            if code in code_map:\n",
        "                directories = code_map[code]\n",
        "                directory = random.choice(directories)\n",
        "                response += \" \"+load_random_reddit(directory, conversation_history)\n",
        "\n",
        "            elif code == \"<GENERATE>\":\n",
        "                response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                              tokenizer,\n",
        "                                              model_reverse,\n",
        "                                              utterance,\n",
        "                                              query_encoding,\n",
        "                                              generated_responses,\n",
        "                                              response_context,\n",
        "                                              conversation_history)\n",
        "            elif code == \"<STOP>\":\n",
        "                stop_flag = 1\n",
        "\n",
        "        elif stop_flag != 1:\n",
        "            mode = \"DEFAULT\"\n",
        "            bias = None\n",
        "\n",
        "            if 'open_question_factual' in labels \\\n",
        "                or ('yes_no_question' in labels and 'NULL' not in labels) \\\n",
        "                or 'open_question_opinion' in labels or 'command' in labels:\n",
        "                bias = 0.07  # biases towards retrieval\n",
        "\n",
        "            elif \"apology\" in labels:\n",
        "                mode = \"BREAK\"\n",
        "                candidates = [\"Apology accepted.\", \"No need to apologize.\",\n",
        "                              \"No worries.\", \"You are forgiven\"]\n",
        "                response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                              tokenizer,\n",
        "                                              model_reverse,\n",
        "                                              utterance,\n",
        "                                              query_encoding,\n",
        "                                              candidates,\n",
        "                                              response_context,\n",
        "                                              conversation_history)\n",
        "\n",
        "            elif \"abandon\" in labels or \"nonsense\" in labels:\n",
        "\n",
        "                mode = np.random.choice([\"BREAK\", \"INITIATE\"], p=[0.6, 0.4])\n",
        "\n",
        "                if mode == \"BREAK\":\n",
        "                    candidates = [\"what?\", \"Can you rephrase what you mean?\",\n",
        "                                  \"What do you mean exactly?\"]\n",
        "                    response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                                  tokenizer,\n",
        "                                                  model_reverse,\n",
        "                                                  utterance,\n",
        "                                                  query_encoding,\n",
        "                                                  generated_responses+candidates,\n",
        "                                                  response_context,\n",
        "                                                  conversation_history)\n",
        "                else:\n",
        "                    directories = code_map['<INITIATE>']\n",
        "                    directory = random.choice(directories)\n",
        "                    response = load_random_reddit(directory, conversation_history)\n",
        "\n",
        "            elif 'hold' in labels:\n",
        "                mode = \"BREAK\"\n",
        "                candidates = [\"Do you want to add something more?\",\n",
        "                              \"I think you want to say something more.\"]\n",
        "                response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                              tokenizer,\n",
        "                                              model_reverse,\n",
        "                                              utterance,\n",
        "                                              query_encoding,\n",
        "                                              generated_responses+candidates,\n",
        "                                              response_context,\n",
        "                                              conversation_history)\n",
        "\n",
        "            elif 'closing' in labels:\n",
        "                mode = \"BREAK\"\n",
        "                candidates = [\"Nice talking to you.\", \"Goodbye.\", \"See you later.\"]\n",
        "                response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                              tokenizer,\n",
        "                                              model_reverse,\n",
        "                                              utterance,\n",
        "                                              query_encoding,\n",
        "                                              candidates,\n",
        "                                              response_context,\n",
        "                                              conversation_history)\n",
        "                stop_flag = 1\n",
        "\n",
        "            elif 'opening' in labels:\n",
        "                mode = \"BREAK\"\n",
        "                response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                              tokenizer,\n",
        "                                              model_reverse,\n",
        "                                              utterance,\n",
        "                                              query_encoding,\n",
        "                                              generated_responses,\n",
        "                                              response_context,\n",
        "                                              conversation_history)\n",
        "                stop_flag = 1\n",
        "\n",
        "            elif 'thanking' in labels:\n",
        "                mode = np.random.choice([\"BREAK\", \"INITIATE\"], p=[0.6, 0.4])\n",
        "\n",
        "                if mode == \"BREAK\":\n",
        "                    candidates = [\"No need to mention\", \"You are welcome.\"]\n",
        "                    response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                                  tokenizer,\n",
        "                                                  model_reverse,\n",
        "                                                  utterance,\n",
        "                                                  query_encoding,\n",
        "                                                  generated_responses+candidates,\n",
        "                                                  response_context,\n",
        "                                                  conversation_history)\n",
        "                else:\n",
        "                    directories = code_map['<INITIATE>']\n",
        "                    directory = random.choice(directories)\n",
        "                    response = load_random_reddit(directory, conversation_history)\n",
        "\n",
        "            elif 'apology' in labels:\n",
        "                mode = \"BREAK\"\n",
        "                candidates = [\"Apology accepted.\", \"Apology granted\",\n",
        "                              \"No Worries!\", \"No need to apologize.\"]\n",
        "                response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                              tokenizer,\n",
        "                                              model_reverse,\n",
        "                                              utterance,\n",
        "                                              query_encoding,\n",
        "                                              generated_responses+candidates,\n",
        "                                              response_context,\n",
        "                                              conversation_history)\n",
        "\n",
        "            elif 'response_to_apology' in labels\\\n",
        "                  or 'pos_answer' in labels or 'neg_answer' in labels\\\n",
        "                  or 'appreciation' in labels or 'back_channeling' in labels:\n",
        "\n",
        "                mode = np.random.choice([\"BREAK\", \"INITIATE\"], p=[0.6, 0.4])\n",
        "\n",
        "                if mode == \"BREAK\":\n",
        "                    response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                                  tokenizer,\n",
        "                                                  model_reverse,\n",
        "                                                  utterance,\n",
        "                                                  query_encoding,\n",
        "                                                  generated_responses,\n",
        "                                                  response_context,\n",
        "                                                  conversation_history)\n",
        "                else:\n",
        "                    directories = code_map['<INITIATE>']\n",
        "                    directory = random.choice(directories)\n",
        "                    response = load_random_reddit(directory, conversation_history)\n",
        "\n",
        "            if mode != \"BREAK\":\n",
        "\n",
        "                chatterbot_cosine_scores = utils.cosine_similarity_nd(\n",
        "                    query_encoding, chatterbot_queries_embd)\n",
        "                chatterbot_queries_, chatterbot_cosine_scores_, _ = top_candidates(\n",
        "                    chatterbot_queries, chatterbot_cosine_scores, top=1)\n",
        "                candidates += chatterbot[chatterbot_queries_[0]]\n",
        "\n",
        "                #print(\"\\n\\nABOUT TO BE RETRIEVED\\n\\n\")\n",
        "\n",
        "                retrieved_candidates = retrieve(\n",
        "                    conn, c, idx, index, query_encoding, query_context)\n",
        "\n",
        "                #print(\"\\n\\nABOUT TO BE RETRIEVED\\n\\n\")\n",
        "\n",
        "                if bias is not None:\n",
        "                    biases = [0.0 for _ in candidates]\n",
        "                    for _ in generated_responses:\n",
        "                        biases.append(0.0)\n",
        "                    for _ in retrieved_candidates:\n",
        "                        biases.append(bias)\n",
        "                    biases = np.asarray(biases, np.float32)\n",
        "                else:\n",
        "                    biases = None\n",
        "\n",
        "                candidates += generated_responses + retrieved_candidates\n",
        "                response, _ = rank_and_choose(USE_QA_model, ConvRT_model,\n",
        "                                              tokenizer,\n",
        "                                              model_reverse,\n",
        "                                              utterance,\n",
        "                                              query_encoding,\n",
        "                                              candidates,\n",
        "                                              response_context,\n",
        "                                              conversation_history,\n",
        "                                              bias=biases)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if len(str(response).split(\" \")) <= 100:\n",
        "            utils.delay_print(\"Bot: \"+response)\n",
        "\n",
        "        else:\n",
        "            utils.delay_print(\"Bot: \"+response, t=0.01)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "        conversation_history.append(utterance)\n",
        "        conversation_history.append(response)\n",
        "\n",
        "        if stop_flag == 1:\n",
        "            break\n",
        "\n",
        "        # break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Say Something: Do you like food?\n",
            "\n",
            "Thinking......"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
